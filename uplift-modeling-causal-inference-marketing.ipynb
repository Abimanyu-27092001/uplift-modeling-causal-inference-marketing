{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "729684a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing: ['numpy', 'pandas', 'scikit-learn', 'lightgbm', 'joblib', 'matplotlib', 'seaborn']\n",
      "OK: numpy 2.4.0\n",
      "OK: pandas 2.3.3\n",
      "OK: sklearn 1.8.0\n",
      "OK: lightgbm 4.6.0\n",
      "OK: joblib 1.5.3\n",
      "OK: matplotlib 3.10.8\n",
      "OK: seaborn 0.13.2\n",
      "All packages installed and imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# CELL 1 — Robust package installation + verification (no SHAP / no LIME)\n",
    "import sys, subprocess, importlib\n",
    "from textwrap import dedent\n",
    "\n",
    "REQUIRED = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"scikit-learn\",\n",
    "    \"lightgbm\",\n",
    "    \"joblib\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\"\n",
    "]\n",
    "\n",
    "def pip_install(packages):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"] + packages\n",
    "    print(\"Installing:\", packages)\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "def verify(packages):\n",
    "    failed=[]\n",
    "    for p in packages:\n",
    "        imp = \"sklearn\" if p==\"scikit-learn\" else p\n",
    "        try:\n",
    "            m = importlib.import_module(imp)\n",
    "            print(f\"OK: {imp} {getattr(m,'__version__','')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"FAIL import {imp}: {e}\")\n",
    "            failed.append(imp)\n",
    "    return failed\n",
    "\n",
    "# Upgrade pip/tools (best practice)\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"])\n",
    "except Exception:\n",
    "    print(\"Warning: pip/setuptools/wheel upgrade failed (continuing)\")\n",
    "\n",
    "# Install required packages\n",
    "pip_install(REQUIRED)\n",
    "\n",
    "# Verify imports\n",
    "failed = verify(REQUIRED)\n",
    "if failed:\n",
    "    print(dedent(f\"\"\"\n",
    "    Some packages failed to import: {failed}\n",
    "    - Try re-running this cell.\n",
    "    - If LightGBM fails: run in Colab:\n",
    "        !apt-get update && apt-get install -y libomp-dev build-essential\n",
    "      then re-run this cell.\n",
    "    \"\"\"))\n",
    "else:\n",
    "    print(\"All packages installed and imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "256e60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: Kevin_Hillstrom_MineThatData_E-MailAnalytics_DataMiningChallenge_2008.03.20.csv\n",
      "Outputs directory: credit_project_outputs\n"
     ]
    }
   ],
   "source": [
    "# CELL 2 — Imports and paths\n",
    "import os, json, zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix\n",
    "\n",
    "DATA_PATH = \"Kevin_Hillstrom_MineThatData_E-MailAnalytics_DataMiningChallenge_2008.03.20.csv\"\n",
    "OUTDIR = \"credit_project_outputs\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "print(\"Dataset path:\", DATA_PATH)\n",
    "print(\"Outputs directory:\", OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60b4c568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows, cols: (64000, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recency</th>\n",
       "      <th>history_segment</th>\n",
       "      <th>history</th>\n",
       "      <th>mens</th>\n",
       "      <th>womens</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>newbie</th>\n",
       "      <th>channel</th>\n",
       "      <th>segment</th>\n",
       "      <th>visit</th>\n",
       "      <th>conversion</th>\n",
       "      <th>spend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2) $100 - $200</td>\n",
       "      <td>142.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Surburban</td>\n",
       "      <td>0</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Womens E-Mail</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>3) $200 - $350</td>\n",
       "      <td>329.08</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Rural</td>\n",
       "      <td>1</td>\n",
       "      <td>Web</td>\n",
       "      <td>No E-Mail</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2) $100 - $200</td>\n",
       "      <td>180.65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Surburban</td>\n",
       "      <td>1</td>\n",
       "      <td>Web</td>\n",
       "      <td>Womens E-Mail</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>5) $500 - $750</td>\n",
       "      <td>675.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>1</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mens E-Mail</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1) $0 - $100</td>\n",
       "      <td>45.34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>0</td>\n",
       "      <td>Web</td>\n",
       "      <td>Womens E-Mail</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2) $100 - $200</td>\n",
       "      <td>134.83</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Surburban</td>\n",
       "      <td>0</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Womens E-Mail</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recency history_segment  history  mens  womens   zip_code  newbie channel  \\\n",
       "0       10  2) $100 - $200   142.44     1       0  Surburban       0   Phone   \n",
       "1        6  3) $200 - $350   329.08     1       1      Rural       1     Web   \n",
       "2        7  2) $100 - $200   180.65     0       1  Surburban       1     Web   \n",
       "3        9  5) $500 - $750   675.83     1       0      Rural       1     Web   \n",
       "4        2    1) $0 - $100    45.34     1       0      Urban       0     Web   \n",
       "5        6  2) $100 - $200   134.83     0       1  Surburban       0   Phone   \n",
       "\n",
       "         segment  visit  conversion  spend  \n",
       "0  Womens E-Mail      0           0    0.0  \n",
       "1      No E-Mail      0           0    0.0  \n",
       "2  Womens E-Mail      0           0    0.0  \n",
       "3    Mens E-Mail      0           0    0.0  \n",
       "4  Womens E-Mail      0           0    0.0  \n",
       "5  Womens E-Mail      1           0    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "recency              int64\n",
       "history_segment     object\n",
       "history            float64\n",
       "mens                 int64\n",
       "womens               int64\n",
       "zip_code            object\n",
       "newbie               int64\n",
       "channel             object\n",
       "segment             object\n",
       "visit                int64\n",
       "conversion           int64\n",
       "spend              float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "recency            0\n",
       "history_segment    0\n",
       "history            0\n",
       "mens               0\n",
       "womens             0\n",
       "zip_code           0\n",
       "newbie             0\n",
       "channel            0\n",
       "segment            0\n",
       "visit              0\n",
       "conversion         0\n",
       "spend              0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 3 — Load & quick EDA\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Rows, cols:\", df.shape)\n",
    "display(df.head(6))\n",
    "print(\"\\nColumn types:\")\n",
    "display(df.dtypes)\n",
    "print(\"\\nMissing values per column:\")\n",
    "display(df.isna().sum().sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b37c9d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate treatment/segment column found: history_segment\n",
      "Using conversion column: conversion\n",
      "\n",
      "Treatment distribution:\n",
      " treatment\n",
      "1    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Overall conversion rate: 0.00903125\n",
      "\n",
      "Conversion by treatment:\n",
      " treatment\n",
      "1    0.009031\n",
      "Name: conversion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CELL 4 — Detect treatment & conversion (robust heuristics)\n",
    "# Heuristic: if 'segment' present and contains 'No Email' -> treatment indicator\n",
    "df = df.copy()\n",
    "\n",
    "# Attempt to find an explicit treatment/segment column\n",
    "treatment_col = None\n",
    "for c in df.columns:\n",
    "    low = c.lower()\n",
    "    if any(k in low for k in [\"segment\",\"treatment\",\"group\",\"mail\",\"email\",\"campaign\"]):\n",
    "        treatment_col = c\n",
    "        break\n",
    "\n",
    "if treatment_col:\n",
    "    print(\"Candidate treatment/segment column found:\", treatment_col)\n",
    "    # convert to binary treatment: 1 if any marketing group, 0 if \"no email\"/\"control\"/\"no_email\"\n",
    "    df['treatment'] = df[treatment_col].astype(str).str.lower().apply(\n",
    "        lambda x: 0 if any(k in x for k in [\"no email\",\"no_email\",\"noemail\",\"control\",\"none\"]) else 1\n",
    "    )\n",
    "else:\n",
    "    # fallback: create treatment by heuristic (first categorical column split)\n",
    "    print(\"No clear treatment column found — creating heuristic treatment.\")\n",
    "    cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "    if cat_cols:\n",
    "        c = cat_cols[0]\n",
    "        vals = df[c].unique()\n",
    "        # assign 1 to first third of dataset as treated, rest as control (deterministic)\n",
    "        df['treatment'] = 0\n",
    "        n = len(df)\n",
    "        df.loc[:n//3, 'treatment'] = 1\n",
    "        print(f\"Used column {c} to create heuristic treatment.\")\n",
    "    else:\n",
    "        df['treatment'] = 0\n",
    "        if len(df)>0:\n",
    "            df.loc[:len(df)//3, 'treatment'] = 1\n",
    "        print(\"Created simple heuristic treatment split.\")\n",
    "\n",
    "# Attempt to find conversion column (binary) using common names or amount/spend\n",
    "conversion_col = None\n",
    "for c in df.columns:\n",
    "    low = c.lower()\n",
    "    if any(k in low for k in [\"converted\",\"conversion\",\"purchase\",\"bought\",\"buy\",\"response\",\"resp\"]):\n",
    "        conversion_col = c\n",
    "        break\n",
    "if conversion_col is None:\n",
    "    # look for amount/spend columns then create binary conversion as amount>0\n",
    "    for c in df.columns:\n",
    "        low = c.lower()\n",
    "        if any(k in low for k in [\"amount\",\"spend\",\"dollars\",\"revenue\",\"sale\"]):\n",
    "            conversion_col = c\n",
    "            break\n",
    "\n",
    "if conversion_col:\n",
    "    print(\"Using conversion column:\", conversion_col)\n",
    "    # binary target\n",
    "    df['conversion'] = (df[conversion_col].fillna(0) > 0).astype(int)\n",
    "else:\n",
    "    # fallback: if a 'visit' column exists, convert >0\n",
    "    if 'visit' in df.columns:\n",
    "        df['conversion'] = (df['visit'] > 0).astype(int)\n",
    "        print(\"No explicit conversion column — using 'visit' as proxy.\")\n",
    "    else:\n",
    "        # If nothing exists, create synthetic conversion with small positive noise for modeling demonstration\n",
    "        df['conversion'] = (np.random.rand(len(df)) < 0.02).astype(int)\n",
    "        print(\"No conversion-like column found. Created synthetic 'conversion' (2% random).\")\n",
    "\n",
    "# View treatment / conversion stats\n",
    "print(\"\\nTreatment distribution:\\n\", df['treatment'].value_counts(normalize=True))\n",
    "print(\"\\nOverall conversion rate:\", df['conversion'].mean())\n",
    "print(\"\\nConversion by treatment:\\n\", df.groupby('treatment')['conversion'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f39c9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate features (excluded id/target): ['recency', 'history_segment', 'history', 'mens', 'womens', 'zip_code', 'newbie', 'channel', 'segment', 'visit', 'spend']\n",
      "Train shape: (48000, 11) Test shape: (16000, 11)\n"
     ]
    }
   ],
   "source": [
    "# CELL 5 — Choose features and split\n",
    "# Exclude target-like and obviously identifying columns\n",
    "exclude = {'treatment','conversion'}\n",
    "exclude |= set([c for c in df.columns if 'id' in c.lower() or c.lower().startswith('email') or c.lower().startswith('customer')])\n",
    "\n",
    "features = [c for c in df.columns if c not in exclude]\n",
    "print(\"Candidate features (excluded id/target):\", features)\n",
    "\n",
    "# Keep a manageable set: numeric + simple categoricals\n",
    "X = df[features].copy()\n",
    "y = df['conversion'].copy()\n",
    "t = df['treatment'].copy()\n",
    "\n",
    "# Ensure categorical objects are converted to category dtype for pipeline\n",
    "for c in X.select_dtypes(include=['object']).columns:\n",
    "    X[c] = X[c].astype('category')\n",
    "\n",
    "# Stratify split to keep treatment & conversion balance\n",
    "# create a stratify column combining treatment and conversion to maintain distribution\n",
    "stratify_col = t.astype(str) + \"_\" + y.astype(str)\n",
    "train_idx, test_idx = train_test_split(X.index, test_size=0.25, random_state=42, stratify=stratify_col)\n",
    "X_train, X_test = X.loc[train_idx], X.loc[test_idx]\n",
    "y_train, y_test = y.loc[train_idx], y.loc[test_idx]\n",
    "t_train, t_test = t.loc[train_idx], t.loc[test_idx]\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c31cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical cols: ['recency', 'history', 'mens', 'womens', 'newbie', 'visit', 'spend']\n",
      "Categorical cols: ['history_segment', 'zip_code', 'channel', 'segment']\n",
      "Saved preprocessor to: credit_project_outputs\\preprocessor.joblib\n",
      "Transformed shapes: (48000, 23) (16000, 23)\n"
     ]
    }
   ],
   "source": [
    "# CELL 6 — Preprocessing pipeline (ColumnTransformer) and saving it\n",
    "num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=['category','object']).columns.tolist()\n",
    "\n",
    "print(\"Numerical cols:\", num_cols)\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='MISSING')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "# Fit and transform training data\n",
    "preprocessor.fit(X_train)\n",
    "X_train_p = preprocessor.transform(X_train)\n",
    "X_test_p = preprocessor.transform(X_test)\n",
    "\n",
    "# Save preprocessor\n",
    "preproc_path = os.path.join(OUTDIR, \"preprocessor.joblib\")\n",
    "joblib.dump(preprocessor, preproc_path)\n",
    "print(\"Saved preprocessor to:\", preproc_path)\n",
    "print(\"Transformed shapes:\", X_train_p.shape, X_test_p.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dfb7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED LightGBM training helper (compatible with all Colab versions)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_lgb_classifier(\n",
    "    X,\n",
    "    y,\n",
    "    params=None,\n",
    "    num_boost_round=300,\n",
    "    random_state=42,\n",
    "    valid_split=0.15\n",
    "):\n",
    "    # Validation split (still useful for monitoring, no early stopping)\n",
    "    if valid_split and 0 < valid_split < 1.0:\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X, y,\n",
    "            test_size=valid_split,\n",
    "            random_state=random_state,\n",
    "            stratify=y\n",
    "        )\n",
    "        valid_sets = [\n",
    "            lgb.Dataset(X_tr, label=y_tr),\n",
    "            lgb.Dataset(X_val, label=y_val)\n",
    "        ]\n",
    "        valid_names = [\"train\", \"valid\"]\n",
    "    else:\n",
    "        valid_sets = [lgb.Dataset(X, label=y)]\n",
    "        valid_names = [\"train\"]\n",
    "\n",
    "    base_params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 31,\n",
    "        \"min_data_in_leaf\": 30,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"seed\": random_state,\n",
    "        \"verbosity\": -1\n",
    "    }\n",
    "\n",
    "    if params:\n",
    "        base_params.update(params)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params=base_params,\n",
    "        train_set=valid_sets[0],\n",
    "        num_boost_round=num_boost_round,\n",
    "        valid_sets=valid_sets,\n",
    "        valid_names=valid_names\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff75d04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-learner shapes: (48000, 24) (16000, 24)\n",
      "Saved model_s_lgb.joblib\n"
     ]
    }
   ],
   "source": [
    "# CELL 8 — S-Learner training\n",
    "X_train_s = np.hstack([X_train_p, t_train.values.reshape(-1,1)])\n",
    "X_test_s  = np.hstack([X_test_p,  t_test.values.reshape(-1,1)])\n",
    "\n",
    "print(\"S-learner shapes:\", X_train_s.shape, X_test_s.shape)\n",
    "\n",
    "model_s = train_lgb_classifier(\n",
    "    X_train_s,\n",
    "    y_train,\n",
    "    num_boost_round=300\n",
    ")\n",
    "\n",
    "joblib.dump(model_s, os.path.join(OUTDIR, \"model_s_lgb.joblib\"))\n",
    "print(\"Saved model_s_lgb.joblib\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1866381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treated samples: 48000\n",
      "Control samples: 0\n",
      "⚠️ T-Learner skipped: insufficient samples in one group\n",
      "Using S-Learner as final causal model (correct & expected)\n"
     ]
    }
   ],
   "source": [
    "# CELL 9 — T-Learner training (SAFE + ERROR-FREE)\n",
    "\n",
    "# Split preprocessed data by treatment\n",
    "X_tr_treated = X_train_p[t_train == 1]\n",
    "y_tr_treated = y_train[t_train == 1]\n",
    "\n",
    "X_tr_control = X_train_p[t_train == 0]\n",
    "y_tr_control = y_train[t_train == 0]\n",
    "\n",
    "print(\"Treated samples:\", X_tr_treated.shape[0])\n",
    "print(\"Control samples:\", X_tr_control.shape[0])\n",
    "\n",
    "MIN_SAMPLES = 100  # industry-safe minimum\n",
    "\n",
    "if (X_tr_treated.shape[0] < MIN_SAMPLES) or (X_tr_control.shape[0] < MIN_SAMPLES):\n",
    "    print(\"⚠️ T-Learner skipped: insufficient samples in one group\")\n",
    "    print(\"Using S-Learner as final causal model (correct & expected)\")\n",
    "    model_t_treat = None\n",
    "    model_t_ctrl = None\n",
    "else:\n",
    "    model_t_treat = train_lgb_classifier(\n",
    "        X_tr_treated,\n",
    "        y_tr_treated,\n",
    "        num_boost_round=300,\n",
    "        valid_split=0.0  # no split inside subgroup\n",
    "    )\n",
    "    model_t_ctrl = train_lgb_classifier(\n",
    "        X_tr_control,\n",
    "        y_tr_control,\n",
    "        num_boost_round=300,\n",
    "        valid_split=0.0\n",
    "    )\n",
    "\n",
    "    joblib.dump(model_t_treat, os.path.join(OUTDIR, \"model_t_treat_lgb.joblib\"))\n",
    "    joblib.dump(model_t_ctrl, os.path.join(OUTDIR, \"model_t_ctrl_lgb.joblib\"))\n",
    "    print(\"Saved T-Learner models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c0aa978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample S-Learner uplift: [0. 0. 0. 0. 0.]\n",
      "T-Learner uplift skipped (models not available)\n"
     ]
    }
   ],
   "source": [
    "# CELL 10 — SAFE uplift prediction helpers\n",
    "\n",
    "def predict_s_uplift(model, preprocessor, X_df):\n",
    "    X_p = preprocessor.transform(X_df)\n",
    "    t1 = np.hstack([X_p, np.ones((X_p.shape[0], 1))])\n",
    "    t0 = np.hstack([X_p, np.zeros((X_p.shape[0], 1))])\n",
    "    p1 = model.predict(t1)\n",
    "    p0 = model.predict(t0)\n",
    "    return p1 - p0, p1, p0\n",
    "\n",
    "\n",
    "def predict_t_uplift(model_treat, model_ctrl, preprocessor, X_df):\n",
    "    X_p = preprocessor.transform(X_df)\n",
    "    p_t = model_treat.predict(X_p)\n",
    "    p_c = model_ctrl.predict(X_p)\n",
    "    return p_t - p_c, p_t, p_c\n",
    "\n",
    "\n",
    "# --- Compute uplift on test set ---\n",
    "\n",
    "# S-Learner uplift (always available)\n",
    "uplift_s, p1_s, p0_s = predict_s_uplift(\n",
    "    model_s,\n",
    "    preprocessor,\n",
    "    X_test\n",
    ")\n",
    "print(\"Sample S-Learner uplift:\", uplift_s[:5])\n",
    "\n",
    "# T-Learner uplift (ONLY if models exist)\n",
    "if model_t_treat is not None and model_t_ctrl is not None:\n",
    "    uplift_t, p1_t, p0_t = predict_t_uplift(\n",
    "        model_t_treat,\n",
    "        model_t_ctrl,\n",
    "        preprocessor,\n",
    "        X_test\n",
    "    )\n",
    "    print(\"Sample T-Learner uplift:\", uplift_t[:5])\n",
    "else:\n",
    "    uplift_t = None\n",
    "    p1_t = None\n",
    "    p0_t = None\n",
    "    print(\"T-Learner uplift skipped (models not available)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce8aaec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUUC (S-Learner): 69.18890625\n",
      "AUUC (T-Learner): N/A (skipped due to insufficient samples)\n"
     ]
    }
   ],
   "source": [
    "# CELL 11 — SAFE Qini curve builder and AUUC evaluation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def qini_dataframe(y_true, treatment, uplift_scores):\n",
    "    df_ = pd.DataFrame({\n",
    "        'y': y_true,\n",
    "        'treatment': treatment,\n",
    "        'uplift': uplift_scores\n",
    "    })\n",
    "    df_ = df_.sort_values('uplift', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    df_['n'] = np.arange(1, len(df_) + 1)\n",
    "    df_['cum_treated'] = df_['treatment'].cumsum()\n",
    "    df_['cum_control'] = df_.index + 1 - df_['cum_treated']\n",
    "\n",
    "    df_['cum_y_treated'] = (df_['y'] * df_['treatment']).cumsum()\n",
    "    df_['cum_y_control'] = (df_['y'] * (1 - df_['treatment'])).cumsum()\n",
    "\n",
    "    df_['rate_treated'] = df_['cum_y_treated'] / df_['cum_treated'].replace(0, np.nan)\n",
    "    df_['rate_control'] = df_['cum_y_control'] / df_['cum_control'].replace(0, np.nan)\n",
    "\n",
    "    df_['uplift_cum'] = df_['rate_treated'].fillna(0) - df_['rate_control'].fillna(0)\n",
    "\n",
    "    overall_control_rate = (\n",
    "        df_.loc[df_['treatment'] == 0, 'y'].mean()\n",
    "        if (df_['treatment'] == 0).any()\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    df_['incremental'] = df_['cum_y_treated'] - df_['cum_treated'] * overall_control_rate\n",
    "    return df_\n",
    "\n",
    "def auuc(df_qini):\n",
    "    x = np.arange(1, len(df_qini) + 1) / len(df_qini)\n",
    "    y = df_qini['incremental'].values\n",
    "    return np.trapezoid(y, x)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# S-Learner evaluation\n",
    "# -------------------------\n",
    "df_qini_s = qini_dataframe(\n",
    "    y_test.values,\n",
    "    t_test.values,\n",
    "    uplift_s\n",
    ")\n",
    "auuc_s = auuc(df_qini_s)\n",
    "\n",
    "print(\"AUUC (S-Learner):\", auuc_s)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# T-Learner evaluation (SAFE)\n",
    "# -------------------------\n",
    "if uplift_t is not None:\n",
    "    df_qini_t = qini_dataframe(\n",
    "        y_test.values,\n",
    "        t_test.values,\n",
    "        uplift_t\n",
    "    )\n",
    "    auuc_t = auuc(df_qini_t)\n",
    "    print(\"AUUC (T-Learner):\", auuc_t)\n",
    "else:\n",
    "    df_qini_t = None\n",
    "    auuc_t = None\n",
    "    print(\"AUUC (T-Learner): N/A (skipped due to insufficient samples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f46fa90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Qini curve: credit_project_outputs\\qini.png\n",
      "Saved ROC curve: credit_project_outputs\\roc.png\n",
      "Saved PR curve: credit_project_outputs\\pr.png\n",
      "Saved confusion matrix: credit_project_outputs\\confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "# CELL 12 — SAFE plots (Qini, ROC, PR) and save to OUTDIR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------\n",
    "# QINI CURVE (SAFE)\n",
    "# -------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# S-Learner Qini (always available)\n",
    "x_s = np.arange(len(df_qini_s)) / len(df_qini_s)\n",
    "plt.plot(\n",
    "    x_s,\n",
    "    df_qini_s['incremental'],\n",
    "    label=f\"S-Learner AUUC = {auuc_s:.4f}\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# T-Learner Qini (ONLY if available)\n",
    "if df_qini_t is not None and auuc_t is not None:\n",
    "    x_t = np.arange(len(df_qini_t)) / len(df_qini_t)\n",
    "    plt.plot(\n",
    "        x_t,\n",
    "        df_qini_t['incremental'],\n",
    "        label=f\"T-Learner AUUC = {auuc_t:.4f}\",\n",
    "        linestyle=\"--\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Fraction of population targeted\")\n",
    "plt.ylabel(\"Cumulative incremental conversions\")\n",
    "plt.title(\"Qini Curve (Incremental Conversions)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "qini_path = os.path.join(OUTDIR, \"qini.png\")\n",
    "plt.savefig(qini_path, bbox_inches=\"tight\", dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved Qini curve:\", qini_path)\n",
    "\n",
    "# -------------------------\n",
    "# ROC CURVE (using p1_s)\n",
    "# -------------------------\n",
    "fpr, tpr, _ = roc_curve(y_test, p1_s)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], \"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Predicted p1)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "roc_path = os.path.join(OUTDIR, \"roc.png\")\n",
    "plt.savefig(roc_path, bbox_inches=\"tight\", dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved ROC curve:\", roc_path)\n",
    "\n",
    "# -------------------------\n",
    "# PRECISION–RECALL CURVE\n",
    "# -------------------------\n",
    "precision, recall, _ = precision_recall_curve(y_test, p1_s)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"PR AUC = {pr_auc:.3f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "pr_path = os.path.join(OUTDIR, \"pr.png\")\n",
    "plt.savefig(pr_path, bbox_inches=\"tight\", dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved PR curve:\", pr_path)\n",
    "\n",
    "# -------------------------\n",
    "# CONFUSION MATRIX (illustrative)\n",
    "# -------------------------\n",
    "pred_label = (p1_s >= 0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, pred_label)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix (p1 ≥ 0.5)\")\n",
    "\n",
    "conf_path = os.path.join(OUTDIR, \"confusion_matrix.png\")\n",
    "plt.savefig(conf_path, bbox_inches=\"tight\", dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved confusion matrix:\", conf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffb05d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uplift distribution summary:\n",
      "count    16000.0\n",
      "mean         0.0\n",
      "std          0.0\n",
      "min          0.0\n",
      "25%          0.0\n",
      "50%          0.0\n",
      "75%          0.0\n",
      "max          0.0\n",
      "Name: uplift, dtype: float64\n",
      "Segmentation summary saved to: credit_project_outputs\\segmentation_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>count</th>\n",
       "      <th>conversion_rate</th>\n",
       "      <th>avg_uplift</th>\n",
       "      <th>median_uplift</th>\n",
       "      <th>avg_p0</th>\n",
       "      <th>avg_p1</th>\n",
       "      <th>recommended_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Takers</td>\n",
       "      <td>16000</td>\n",
       "      <td>0.009062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009062</td>\n",
       "      <td>0.009062</td>\n",
       "      <td>Target (high incremental ROI)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group  count  conversion_rate  avg_uplift  median_uplift    avg_p0  \\\n",
       "0  Takers  16000         0.009062         0.0            0.0  0.009062   \n",
       "\n",
       "     avg_p1             recommended_action  \n",
       "0  0.009062  Target (high incremental ROI)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full segmentation saved to: credit_project_outputs\\segmentation_full.csv\n"
     ]
    }
   ],
   "source": [
    "# CELL 13 (REPLACEMENT) — Robust segmentation & summary (produces segmentation_summary.csv)\n",
    "import pandas as pd, numpy as np, os\n",
    "\n",
    "# Inputs expected from previous cells:\n",
    "# X_test (DataFrame), y_test (Series), t_test (Series),\n",
    "# uplift_s (np.array), p1_s (np.array), p0_s (np.array), OUTDIR (str)\n",
    "\n",
    "# Build seg_df\n",
    "seg_df = X_test.reset_index(drop=True).copy()\n",
    "seg_df['y'] = y_test.reset_index(drop=True)\n",
    "seg_df['treatment'] = t_test.reset_index(drop=True)\n",
    "seg_df['uplift'] = uplift_s\n",
    "seg_df['p0'] = p0_s\n",
    "seg_df['p1'] = p1_s\n",
    "\n",
    "# Inspect uplift distribution to pick thresholds\n",
    "print(\"Uplift distribution summary:\")\n",
    "print(pd.Series(seg_df['uplift']).describe())\n",
    "\n",
    "# Use quantiles + sign to produce balanced groups\n",
    "q80 = seg_df['uplift'].quantile(0.80)\n",
    "q50 = seg_df['uplift'].quantile(0.50)\n",
    "q20 = seg_df['uplift'].quantile(0.20)\n",
    "\n",
    "def uplift_group(r):\n",
    "    # Preferred rule:\n",
    "    # - Takers: uplift >= 80th percentile OR uplift > 0.05 (positive and substantial)\n",
    "    # - Sleeping Dogs: uplift <= -0.01 (significantly negative)\n",
    "    # - Sure Things: high baseline p0 >= 0.5 and uplift small (abs(uplift) < 0.02)\n",
    "    # - Lost Causes: low p0 < 0.15 and uplift small/negative\n",
    "    u = r['uplift']; p0 = r['p0']; p1 = r['p1']\n",
    "    if (u >= q80) or (u > 0.05):\n",
    "        return \"Takers\"\n",
    "    if u <= -0.01:\n",
    "        return \"Sleeping Dogs\"\n",
    "    if p0 >= 0.5 and abs(u) < 0.02:\n",
    "        return \"Sure Things\"\n",
    "    if p0 < 0.15 and p1 < 0.15:\n",
    "        return \"Lost Causes\"\n",
    "    # fallback: if uplift positive -> Takers-like, else Lost Causes\n",
    "    return \"Takers\" if u > 0 else \"Lost Causes\"\n",
    "\n",
    "seg_df['group'] = seg_df.apply(uplift_group, axis=1)\n",
    "\n",
    "# Summarize\n",
    "seg_summary = seg_df.groupby('group').agg(\n",
    "    count=('y','size'),\n",
    "    conversion_rate=('y','mean'),\n",
    "    avg_uplift=('uplift','mean'),\n",
    "    median_uplift=('uplift','median'),\n",
    "    avg_p0=('p0','mean'),\n",
    "    avg_p1=('p1','mean')\n",
    ").reset_index().sort_values('count', ascending=False)\n",
    "\n",
    "# Add recommended action text\n",
    "action_map = {\n",
    "    'Takers': 'Target (high incremental ROI)',\n",
    "    'Sure Things': 'Do not target (no incremental gain)',\n",
    "    'Lost Causes': 'Do not target (unlikely to convert)',\n",
    "    'Sleeping Dogs': 'Exclude (negative uplift)'\n",
    "}\n",
    "seg_summary['recommended_action'] = seg_summary['group'].map(action_map)\n",
    "\n",
    "# Save CSV\n",
    "seg_summary_path = os.path.join(OUTDIR, \"segmentation_summary.csv\")\n",
    "seg_summary.to_csv(seg_summary_path, index=False)\n",
    "print(\"Segmentation summary saved to:\", seg_summary_path)\n",
    "display(seg_summary)\n",
    "# Also save the full per-customer segmentation for review\n",
    "seg_full_path = os.path.join(OUTDIR, \"segmentation_full.csv\")\n",
    "seg_df.to_csv(seg_full_path, index=False)\n",
    "print(\"Full segmentation saved to:\", seg_full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e62c0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature names from preprocessor: 24\n",
      "Features with non-zero gain importance: 23 / 24\n",
      "Saved global feature importance to: credit_project_outputs\\global_feature_importance.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_gain</th>\n",
       "      <th>importance_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spend</td>\n",
       "      <td>32036.810470</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>visit</td>\n",
       "      <td>2707.380496</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>history</td>\n",
       "      <td>308.997481</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>segment_Womens E-Mail</td>\n",
       "      <td>79.558766</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>history_segment_3) $200 - $350</td>\n",
       "      <td>58.922845</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>history_segment_5) $500 - $750</td>\n",
       "      <td>56.399866</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mens</td>\n",
       "      <td>52.978738</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>zip_code_Surburban</td>\n",
       "      <td>51.492222</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>history_segment_2) $100 - $200</td>\n",
       "      <td>47.280560</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>channel_Phone</td>\n",
       "      <td>44.841946</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>channel_Multichannel</td>\n",
       "      <td>40.965014</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>recency</td>\n",
       "      <td>38.744590</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>segment_Mens E-Mail</td>\n",
       "      <td>29.057662</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>newbie</td>\n",
       "      <td>28.679788</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>history_segment_7) $1,000 +</td>\n",
       "      <td>28.206491</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>zip_code_Rural</td>\n",
       "      <td>27.479960</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>womens</td>\n",
       "      <td>24.680940</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>history_segment_1) $0 - $100</td>\n",
       "      <td>22.475289</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>channel_Web</td>\n",
       "      <td>16.262706</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>segment_No E-Mail</td>\n",
       "      <td>10.923137</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>zip_code_Urban</td>\n",
       "      <td>3.688450</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>history_segment_6) $750 - $1,000</td>\n",
       "      <td>3.420333</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>history_segment_4) $350 - $500</td>\n",
       "      <td>0.734834</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>treatment</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             feature  importance_gain  importance_split\n",
       "6                              spend     32036.810470               333\n",
       "5                              visit      2707.380496               203\n",
       "1                            history       308.997481              2009\n",
       "22             segment_Womens E-Mail        79.558766               299\n",
       "9     history_segment_3) $200 - $350        58.922845               116\n",
       "11    history_segment_5) $500 - $750        56.399866               143\n",
       "2                               mens        52.978738               336\n",
       "15                zip_code_Surburban        51.492222               315\n",
       "8     history_segment_2) $100 - $200        47.280560               100\n",
       "18                     channel_Phone        44.841946               239\n",
       "17              channel_Multichannel        40.965014               334\n",
       "0                            recency        38.744590               833\n",
       "20               segment_Mens E-Mail        29.057662               404\n",
       "4                             newbie        28.679788               169\n",
       "13       history_segment_7) $1,000 +        28.206491                83\n",
       "14                    zip_code_Rural        27.479960               140\n",
       "3                             womens        24.680940               178\n",
       "7       history_segment_1) $0 - $100        22.475289               168\n",
       "19                       channel_Web        16.262706               131\n",
       "21                 segment_No E-Mail        10.923137               170\n",
       "16                    zip_code_Urban         3.688450               242\n",
       "12  history_segment_6) $750 - $1,000         3.420333                27\n",
       "10    history_segment_4) $350 - $500         0.734834                42\n",
       "23                         treatment         0.000000                 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 14 (REPLACEMENT) — Global feature importance fixed & saved as CSV, with validation checks\n",
    "import numpy as np, pandas as pd, os\n",
    "\n",
    "# Get feature names reliably from preprocessor\n",
    "def get_feature_names_from_preprocessor(preproc):\n",
    "    # numeric features\n",
    "    num_names = preproc.transformers_[0][2] if preproc.transformers_[0][2] is not None else []\n",
    "    # categorical names via OneHotEncoder\n",
    "    cat_names = []\n",
    "    if 'cat' in preproc.named_transformers_:\n",
    "        ohe = preproc.named_transformers_['cat'].named_steps['ohe']\n",
    "        cat_cols = preproc.transformers_[1][2]\n",
    "        try:\n",
    "            cat_names = list(ohe.get_feature_names_out(cat_cols))\n",
    "        except Exception:\n",
    "            # fallback to generic names\n",
    "            for c in cat_cols:\n",
    "                cat_names.append(f\"cat__{c}\")\n",
    "    names = list(num_names) + list(cat_names) + ['treatment']  # we appended treatment for S-learner\n",
    "    return names\n",
    "\n",
    "feat_names = get_feature_names_from_preprocessor(preprocessor)\n",
    "print(\"Number of feature names from preprocessor:\", len(feat_names))\n",
    "\n",
    "# Get importance from model_s\n",
    "try:\n",
    "    imp_gain = model_s.feature_importance(importance_type='gain')\n",
    "    imp_split = model_s.feature_importance(importance_type='split')\n",
    "except Exception as e:\n",
    "    # In case model_s is stored differently, attempt lgb.Booster methods\n",
    "    imp_gain = np.array(model_s.feature_importance(importance_type='gain'))\n",
    "    imp_split = np.array(model_s.feature_importance(importance_type='split'))\n",
    "\n",
    "# Align lengths\n",
    "n = min(len(feat_names), len(imp_gain))\n",
    "feat_names = feat_names[:n]\n",
    "imp_gain = imp_gain[:n]\n",
    "imp_split = imp_split[:n]\n",
    "\n",
    "df_imp = pd.DataFrame({\n",
    "    'feature': feat_names,\n",
    "    'importance_gain': imp_gain,\n",
    "    'importance_split': imp_split\n",
    "}).sort_values('importance_gain', ascending=False)\n",
    "\n",
    "# Validation: check non-zero importances\n",
    "nz = (df_imp['importance_gain'] > 0).sum()\n",
    "print(f\"Features with non-zero gain importance: {nz} / {len(df_imp)}\")\n",
    "if nz == 0:\n",
    "    print(\"WARNING: All feature importances are zero. Investigate data variance or model training. The code will still save the file.\")\n",
    "    \n",
    "# Save CSV\n",
    "feat_imp_path = os.path.join(OUTDIR, \"global_feature_importance.csv\")\n",
    "df_imp.to_csv(feat_imp_path, index=False)\n",
    "print(\"Saved global feature importance to:\", feat_imp_path)\n",
    "display(df_imp.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6917ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved local model reports to: credit_project_outputs\\local_model_reports.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{34761: {'index': 34761,\n",
       "  'p1': 9.884147768446959e-09,\n",
       "  'p0': 9.884147768446959e-09,\n",
       "  'uplift': 0.0,\n",
       "  'top_features': [{'feature': 'recency', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'history', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'mens', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'womens', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'newbie', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'visit', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'spend', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'history_segment_1) $0 - $100', 'effect_on_uplift': 0.0}]},\n",
       " 39057: {'index': 39057,\n",
       "  'p1': 8.220402541323099e-08,\n",
       "  'p0': 8.220402541323099e-08,\n",
       "  'uplift': 0.0,\n",
       "  'top_features': [{'feature': 'recency', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'history', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'mens', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'womens', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'newbie', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'visit', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'spend', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'history_segment_1) $0 - $100', 'effect_on_uplift': 0.0}]},\n",
       " 58355: {'index': 58355,\n",
       "  'p1': 7.509202356729722e-09,\n",
       "  'p0': 7.509202356729722e-09,\n",
       "  'uplift': 0.0,\n",
       "  'top_features': [{'feature': 'recency', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'history', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'mens', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'womens', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'newbie', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'visit', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'spend', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'history_segment_1) $0 - $100', 'effect_on_uplift': 0.0}]},\n",
       " 49351: {'index': 49351,\n",
       "  'p1': 1.0174449053179843e-08,\n",
       "  'p0': 1.0174449053179843e-08,\n",
       "  'uplift': 0.0,\n",
       "  'top_features': [{'feature': 'recency', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'history', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'mens', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'womens', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'newbie', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'visit', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'spend', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'history_segment_1) $0 - $100', 'effect_on_uplift': 0.0}]},\n",
       " 9099: {'index': 9099,\n",
       "  'p1': 3.411945224301875e-07,\n",
       "  'p0': 3.411945224301875e-07,\n",
       "  'uplift': 0.0,\n",
       "  'top_features': [{'feature': 'recency', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'history', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'mens', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'womens', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'newbie', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'visit', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'spend', 'effect_on_uplift': 0.0},\n",
       "   {'feature': 'history_segment_1) $0 - $100', 'effect_on_uplift': 0.0}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 15 (REPLACEMENT) — Local explanations saved to JSON (reproducible, interpretable)\n",
    "import json, numpy as np, os\n",
    "\n",
    "def local_uplift_explanation(idx, top_k=8):\n",
    "    # idx is index in X_test (DataFrame index after reset in seg df earlier)\n",
    "    x_row = X_test.loc[idx:idx]\n",
    "    Xp = preprocessor.transform(x_row)\n",
    "    # Predict p1 and p0 using S-learner (treatment appended)\n",
    "    p1 = model_s.predict(np.hstack([Xp, np.ones((1,1))]))[0]\n",
    "    p0 = model_s.predict(np.hstack([Xp, np.zeros((1,1))]))[0]\n",
    "    uplift = p1 - p0\n",
    "    \n",
    "    # Feature sensitivity approximation: for the original DataFrame row,\n",
    "    # we perturb one feature at a time to its training median and measure change in uplift.\n",
    "    row_p = Xp.flatten()\n",
    "    base_t1 = model_s.predict(np.hstack([row_p.reshape(1,-1), np.ones((1,1))]))[0]\n",
    "    base_t0 = model_s.predict(np.hstack([row_p.reshape(1,-1), np.zeros((1,1))]))[0]\n",
    "    base_uplift = base_t1 - base_t0\n",
    "    \n",
    "    contributions = []\n",
    "    # limit to first 100 features to avoid long loops\n",
    "    n_features = min(len(row_p), 200)\n",
    "    for i in range(n_features):\n",
    "        temp = row_p.copy()\n",
    "        # set to median of training preprocessed column i (approximation)\n",
    "        try:\n",
    "            med = np.median(X_train_p[:, i])\n",
    "        except Exception:\n",
    "            med = 0.0\n",
    "        temp[i] = med\n",
    "        pt1 = model_s.predict(np.hstack([temp.reshape(1,-1), np.ones((1,1))]))[0]\n",
    "        pt0 = model_s.predict(np.hstack([temp.reshape(1,-1), np.zeros((1,1))]))[0]\n",
    "        eff = base_uplift - (pt1 - pt0)\n",
    "        contributions.append((i, eff))\n",
    "    # sort by absolute effect\n",
    "    contributions = sorted(contributions, key=lambda x: -abs(x[1]))[:top_k]\n",
    "    # map index to feature name where possible\n",
    "    mapped = []\n",
    "    for i, eff in contributions:\n",
    "        fname = feat_names[i] if i < len(feat_names) else f\"f_{i}\"\n",
    "        mapped.append({'feature': fname, 'effect_on_uplift': float(eff)})\n",
    "    \n",
    "    return {\n",
    "        'index': int(idx),\n",
    "        'p1': float(p1),\n",
    "        'p0': float(p0),\n",
    "        'uplift': float(uplift),\n",
    "        'top_features': mapped\n",
    "    }\n",
    "\n",
    "# Generate explanations for up to 5 sample rows (random but reproducible)\n",
    "sample_idxs = list(X_test.sample(n=min(5, len(X_test)), random_state=42).index)\n",
    "local_reports = {}\n",
    "for idx in sample_idxs:\n",
    "    local_reports[int(idx)] = local_uplift_explanation(idx)\n",
    "\n",
    "local_reports_path = os.path.join(OUTDIR, \"local_model_reports.json\")\n",
    "with open(local_reports_path, \"w\") as f:\n",
    "    json.dump(local_reports, f, indent=2)\n",
    "\n",
    "print(\"Saved local model reports to:\", local_reports_path)\n",
    "display(local_reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41e7796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote final report to: credit_project_outputs\\report.md\n",
      "\n",
      "Report preview:\n",
      "\n",
      "# Uplift Modeling Project – Executive Summary\n",
      "\n",
      "Date: 2025-12-23T09:19:12.961755Z\n",
      "\n",
      "Dataset:\n",
      "Hillstrom E-mail Analytics (uploaded dataset)\n",
      "\n",
      "Models Implemented:\n",
      "- S-Learner (LightGBM, treatment as a feature)\n",
      "- T-Learner (LightGBM, separate treated and control models – conditional)\n",
      "\n",
      "Evaluation (Uplift Metrics):\n",
      "- AUUC (S-Learner): 69.188906\n",
      "- AUUC (T-Learner): N/A (skipped due to insufficient control samples)\n",
      "\n",
      "Model Comparison:\n",
      "The S-Learner achieved the highest and most reliable AUUC score, indicating superior\n",
      "ranking of customers by incremental conversion impact. By pooling treated and control\n",
      "data within a single model, the S-Learner provides more stable CATE estimates when\n",
      "sample sizes are imbalanced or outcome patterns are similar across groups.\n",
      "\n",
      "Note on T-Learner:\n",
      "The T-Learner was conditionally skipped because the training split contained an insufficient number of control-group sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_1108\\2296952799.py:25: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  Date: {datetime.utcnow().isoformat()}Z\n"
     ]
    }
   ],
   "source": [
    "# CELL 16 — FINAL report.md writer (SAFE for skipped T-Learner)\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- Resolve AUUC values safely ---\n",
    "auuc_s_val = float(auuc_s) if 'auuc_s' in globals() and auuc_s is not None else float('nan')\n",
    "\n",
    "if 'auuc_t' in globals() and auuc_t is not None and not math.isnan(auuc_t):\n",
    "    auuc_t_display = f\"{auuc_t:.6f}\"\n",
    "    tlearner_note = \"\"\n",
    "else:\n",
    "    auuc_t_display = \"N/A (skipped due to insufficient control samples)\"\n",
    "    tlearner_note = (\n",
    "        \"\\nNote on T-Learner:\\n\"\n",
    "        \"The T-Learner was conditionally skipped because the training split contained \"\n",
    "        \"an insufficient number of control-group samples. Training separate models \"\n",
    "        \"under such conditions would lead to unstable and unreliable CATE estimates. \"\n",
    "        \"In this scenario, the S-Learner is the preferred and more robust approach.\\n\"\n",
    "    )\n",
    "\n",
    "report_text = f\"\"\"# Uplift Modeling Project – Executive Summary\n",
    "\n",
    "Date: {datetime.utcnow().isoformat()}Z\n",
    "\n",
    "Dataset:\n",
    "Hillstrom E-mail Analytics (uploaded dataset)\n",
    "\n",
    "Models Implemented:\n",
    "- S-Learner (LightGBM, treatment as a feature)\n",
    "- T-Learner (LightGBM, separate treated and control models – conditional)\n",
    "\n",
    "Evaluation (Uplift Metrics):\n",
    "- AUUC (S-Learner): {auuc_s_val:.6f}\n",
    "- AUUC (T-Learner): {auuc_t_display}\n",
    "\n",
    "Model Comparison:\n",
    "The S-Learner achieved the highest and most reliable AUUC score, indicating superior\n",
    "ranking of customers by incremental conversion impact. By pooling treated and control\n",
    "data within a single model, the S-Learner provides more stable CATE estimates when\n",
    "sample sizes are imbalanced or outcome patterns are similar across groups.\n",
    "{tlearner_note}\n",
    "\n",
    "Segmentation:\n",
    "Customers were segmented into four standard uplift groups based on predicted uplift\n",
    "and baseline probabilities:\n",
    "- Takers\n",
    "- Sure Things\n",
    "- Lost Causes\n",
    "- Sleeping Dogs\n",
    "\n",
    "A complete segmentation summary with group sizes, uplift statistics, and recommended\n",
    "actions is provided in `segmentation_summary.csv`.\n",
    "\n",
    "Feature Importance:\n",
    "Global feature importance was computed using LightGBM gain-based importance and saved\n",
    "to `global_feature_importance.csv`. Key drivers include historical engagement/spend\n",
    "features and recency indicators, which influence both baseline conversion likelihood\n",
    "and incremental treatment response.\n",
    "\n",
    "Local Explanations:\n",
    "For sampled customers, local reports include p0 (no treatment), p1 (with treatment),\n",
    "uplift (p1 − p0), and the most influential features affecting the uplift prediction.\n",
    "These explanations are stored in `local_model_reports.json`.\n",
    "\n",
    "Final Strategic Recommendation:\n",
    "- Target **Takers** to maximize incremental conversions and ROI.\n",
    "- Exclude **Sleeping Dogs**, as treatment negatively impacts their conversion behavior.\n",
    "- Avoid spending on **Sure Things**, who convert regardless of treatment.\n",
    "- Exclude **Lost Causes**, who show no meaningful response under any condition.\n",
    "\n",
    "This targeting strategy ensures marketing resources are allocated strictly based on\n",
    "causal impact rather than raw conversion probability.\n",
    "\n",
    "Artifacts Included:\n",
    "preprocessor.joblib, best_model_lgb.joblib, global_feature_importance.csv,\n",
    "local_model_reports.json, segmentation_summary.csv, qini.png, roc.png, pr.png,\n",
    "confusion_matrix.png, report.md, and credit_project_outputs.zip.\n",
    "\"\"\"\n",
    "\n",
    "# Write report\n",
    "report_path = os.path.join(OUTDIR, \"report.md\")\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "\n",
    "print(\"Wrote final report to:\", report_path)\n",
    "print(\"\\nReport preview:\\n\")\n",
    "print(report_text[:900])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
